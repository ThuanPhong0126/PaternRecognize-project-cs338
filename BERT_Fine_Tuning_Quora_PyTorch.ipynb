{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "BERT-Fine-Tuning-Quora-PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThuanPhong0126/PaternRecognize-project-cs338/blob/main/BERT_Fine_Tuning_Quora_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi4Kce0x9Fnt",
        "outputId": "7689ddc9-2216-4981-ad4d-cd0d9efc4eb7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG-OKEmJ98Uh",
        "outputId": "f5da3816-fe93-405d-dc9d-f23f2da55fcd"
      },
      "source": [
        "cd /content/gdrive/MyDrive/[]Nhan Dang/Question-Similarity"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/[]Nhan Dang/Question-Similarity\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "k1sJMM5S8L4w",
        "outputId": "82f81d6b-66bb-47af-9999-ac31e4f3d850"
      },
      "source": [
        "import pandas\n",
        "\n",
        "train = pandas.read_csv(\"./Quora dataset/train.csv\", index_col='id')\n",
        "dev = pandas.read_csv(\"./Quora dataset/dev.csv\", index_col='id')\n",
        "test = pandas.read_csv(\"./Quora dataset/test.csv\", index_col='id')\n",
        "\n",
        "X_train = train[['question1', 'question2']]\n",
        "X_validation = dev[['question1', 'question2']]\n",
        "X_test = test[['question1', 'question2']]\n",
        "\n",
        "y_train = train[['is_duplicate']]\n",
        "y_validation = dev[['is_duplicate']]\n",
        "y_test = test[['is_duplicate']]\n",
        "\n",
        "X_train"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
              "      <td>What would happen if the Indian government sto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can I increase the speed of my internet co...</td>\n",
              "      <td>How can Internet speed be increased by hacking...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
              "      <td>Which fish would survive in salt water?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
              "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>When do you use シ instead of し?</td>\n",
              "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404284</th>\n",
              "      <td>What does Jainism say about homosexuality?</td>\n",
              "      <td>What does Jainism say about Gays and Homosexua...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404285</th>\n",
              "      <td>How many keywords are there in the Racket prog...</td>\n",
              "      <td>How many keywords are there in PERL Programmin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404286</th>\n",
              "      <td>Do you believe there is life after death?</td>\n",
              "      <td>Is it true that there is life after death?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404288</th>\n",
              "      <td>What is the approx annual cost of living while...</td>\n",
              "      <td>I am having little hairfall problem but I want...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404289</th>\n",
              "      <td>What is like to have sex with cousin?</td>\n",
              "      <td>What is it like to have sex with your cousin?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>244290 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                question1                                          question2\n",
              "id                                                                                                          \n",
              "1       What is the story of Kohinoor (Koh-i-Noor) Dia...  What would happen if the Indian government sto...\n",
              "2       How can I increase the speed of my internet co...  How can Internet speed be increased by hacking...\n",
              "4       Which one dissolve in water quikly sugar, salt...            Which fish would survive in salt water?\n",
              "5       Astrology: I am a Capricorn Sun Cap moon and c...  I'm a triple Capricorn (Sun, Moon and ascendan...\n",
              "8                         When do you use シ instead of し?              When do you use \"&\" instead of \"and\"?\n",
              "...                                                   ...                                                ...\n",
              "404284         What does Jainism say about homosexuality?  What does Jainism say about Gays and Homosexua...\n",
              "404285  How many keywords are there in the Racket prog...  How many keywords are there in PERL Programmin...\n",
              "404286          Do you believe there is life after death?         Is it true that there is life after death?\n",
              "404288  What is the approx annual cost of living while...  I am having little hairfall problem but I want...\n",
              "404289              What is like to have sex with cousin?      What is it like to have sex with your cousin?\n",
              "\n",
              "[244290 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQdJKIAN-Hpr"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q585V7ff8L45"
      },
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', do_lower_case=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDr2zvOy8L5H"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "X_train[\"question1_length\"] = X_train[\"question1\"].progress_apply(lambda question: \n",
        "                                                                                      len(tokenizer.tokenize(question)))\n",
        "X_train[\"question2_length\"] = X_train[\"question2\"].progress_apply(lambda question: \n",
        "                                                                                      len(tokenizer.tokenize(question)))\n",
        "X_train[\"joint_length\"] = X_train[\"question1_length\"] + X_train[\"question2_length\"]\n",
        "X_train[\"joint_length\"].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjjncMnI8L5R"
      },
      "source": [
        "max_length = 310\n",
        "tokenizer.encode_plus(X_train.iloc[0][\"question1\"], X_train.iloc[0][\"question2\"], max_length=max_length, \n",
        "                      pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkI3kcDR8L5U"
      },
      "source": [
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "\n",
        "\n",
        "def convert_to_dataset_torch(data: pandas.DataFrame, labels: pandas.Series) -> TensorDataset:\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    token_type_ids = []\n",
        "    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
        "        encoded_dict = tokenizer.encode_plus(row[\"question1\"], row[\"question2\"], max_length=max_length, pad_to_max_length=True, \n",
        "                      return_attention_mask=True, return_tensors='pt', truncation=True)\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        token_type_ids.append(encoded_dict[\"token_type_ids\"])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "    \n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels.values)\n",
        "    \n",
        "    return TensorDataset(input_ids, attention_masks, token_type_ids, labels)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ykxkur-8L5W",
        "outputId": "368f1818-4896-4775-cc3c-356247fc6aab"
      },
      "source": [
        "train = convert_to_dataset_torch(X_train, y_train)\n",
        "validation = convert_to_dataset_torch(X_validation, y_validation)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1920 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 1920/1920 [00:00<00:00, 1930.37it/s]\n",
            "100%|██████████| 480/480 [00:00<00:00, 1859.25it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs_yDBpQ8L5Y"
      },
      "source": [
        "import multiprocessing\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "core_number = multiprocessing.cpu_count()\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train, \n",
        "            sampler = RandomSampler(train), \n",
        "            batch_size = batch_size,\n",
        "            num_workers = core_number\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            validation,\n",
        "            sampler = SequentialSampler(validation), \n",
        "            batch_size = batch_size,\n",
        "            num_workers = core_number\n",
        "        )"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEUm7M_A8L5b",
        "outputId": "c7828c65-544a-4937-b228-9f00cdb52f56"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "bert_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-large-uncased\",  # bert-base-uncased\n",
        "    num_labels=2,\n",
        "               \n",
        "    output_attentions=False, \n",
        "    output_hidden_states=False, \n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpJujH5E8L5h"
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "adamw_optimizer = AdamW(bert_model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn0TXYdd8L5j"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(adamw_optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0l6wb6d8L5l"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8nZOre08L5n"
      },
      "source": [
        "def fit_batch(dataloader, model, optimizer, epoch):\n",
        "    total_train_loss = 0\n",
        "    \n",
        "    for batch in tqdm(dataloader, desc=f\"Training epoch:{epoch}\", unit=\"batch\"):\n",
        "        input_ids, attention_masks, token_type_ids, labels = batch\n",
        "\n",
        "        model.zero_grad()\n",
        "        \n",
        "        loss = model(input_ids, \n",
        "                             token_type_ids=token_type_ids, \n",
        "                             attention_mask=attention_masks, \n",
        "                             labels=labels)\n",
        "        loss = loss['loss']\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        \n",
        "    return total_train_loss"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMD0YeVU8L5r"
      },
      "source": [
        "import numpy\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval_batch(dataloader, model, metric=accuracy_score):\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    predictions , predicted_labels = [], []\n",
        "    \n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
        "        input_ids, attention_masks, token_type_ids, labels = batch\n",
        "        \n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss = model(input_ids, \n",
        "                                   token_type_ids=token_type_ids, \n",
        "                                   attention_mask=attention_masks,\n",
        "                                   labels=labels)\n",
        "            logits = loss['logits']\n",
        "            loss = loss['loss']\n",
        "        total_eval_loss += loss.item()\n",
        "        \n",
        "        y_pred = numpy.argmax(logits.detach().numpy(), axis=1).flatten()\n",
        "        total_eval_accuracy += metric(labels, y_pred)\n",
        "        \n",
        "        predictions.extend(logits.detach().numpy().tolist())\n",
        "        predicted_labels.extend(y_pred.tolist())\n",
        "    \n",
        "    return total_eval_accuracy, total_eval_loss, predictions ,predicted_labels"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yv_6Lc_8L5u"
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "numpy.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "\n",
        "\n",
        "def train(train_dataloader, validation_dataloader, model, optimizer, epochs):\n",
        "    training_stats = []\n",
        "    \n",
        "    total_t0 = time.time()\n",
        "    \n",
        "    for epoch in range(0, epochs):\n",
        "        \n",
        "        t0 = time.time()\n",
        "        \n",
        "        total_train_loss = 0\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        total_train_loss = fit_batch(train_dataloader, model, optimizer, epoch)\n",
        "        \n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        \n",
        "        training_time = format_time(time.time() - t0)\n",
        "        \n",
        "        t0 = time.time()\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "        total_eval_accuracy, total_eval_loss, _, _ = eval_batch(validation_dataloader, model)\n",
        "        \n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        \n",
        "        print(f\"  Accuracy: {avg_val_accuracy}\")\n",
        "    \n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "        validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "        print(f\"  Validation Loss: {avg_val_loss}\")\n",
        "    \n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "                'Valid. Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time\n",
        "            }\n",
        "        )\n",
        "        \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    print(f\"Total training took {format_time(time.time()-total_t0)}\")\n",
        "    return training_stats"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1biAw4xF8L5z"
      },
      "source": [
        "training_stats = train(train_dataloader, validation_dataloader, bert_model, adamw_optimizer, epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXTOv-C78L51"
      },
      "source": [
        "df_stats = pandas.DataFrame(training_stats).set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCYFIuVR8L52"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "pyplot.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "pyplot.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "pyplot.title(\"Training & Validation Loss\")\n",
        "pyplot.xlabel(\"Epoch\")\n",
        "pyplot.ylabel(\"Loss\")\n",
        "pyplot.legend()\n",
        "pyplot.xticks(df_stats.index.values.tolist())\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2-qnhdz8L54"
      },
      "source": [
        "# Performance On Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd9_cuvS8L55",
        "outputId": "bcdc9c1f-cc44-4093-c747-49cec21034b0"
      },
      "source": [
        "test = convert_to_dataset_torch(X_test, y_test)\n",
        "test_dataloader = DataLoader(test,  sampler=SequentialSampler(test), batch_size=batch_size)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/600 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "100%|██████████| 600/600 [00:00<00:00, 1860.58it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxsXu9gi8L58",
        "outputId": "9874d1ae-b150-4145-abc4-75cdc8d7b6da"
      },
      "source": [
        "bert_model.eval()\n",
        "\n",
        "_, _,_ ,predicted_labels = eval_batch(test_dataloader, bert_model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 75/75 [18:11<00:00, 14.56s/batch]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrkbwW4o8L6C",
        "outputId": "c5e58132-2128-4c74-f1a6-23c78caed575"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "output_dir = Path(\"__file__\").parents[0].absolute().joinpath(\"bert_large\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = bert_model.module if hasattr(bert_model, 'module') else bert_model \n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(str(output_dir.absolute()))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/gdrive/My Drive/[]Nhan Dang/Question-Similarity/bert_large/tokenizer_config.json',\n",
              " '/content/gdrive/My Drive/[]Nhan Dang/Question-Similarity/bert_large/special_tokens_map.json',\n",
              " '/content/gdrive/My Drive/[]Nhan Dang/Question-Similarity/bert_large/vocab.txt',\n",
              " '/content/gdrive/My Drive/[]Nhan Dang/Question-Similarity/bert_large/added_tokens.json',\n",
              " '/content/gdrive/My Drive/[]Nhan Dang/Question-Similarity/bert_large/tokenizer.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}